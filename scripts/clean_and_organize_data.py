#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸…æ´—å’Œæ•´ç†Excelé»‘åå•æ•°æ®
åŸºäºåˆ†æç»“æœå¯¹æ•°æ®è¿›è¡Œæ¸…æ´—ã€å»é‡å’Œæ ‡å‡†åŒ–å¤„ç†
"""

import pandas as pd
import re
import json
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import numpy as np

def clean_text(text: Any) -> Optional[str]:
    """æ¸…ç†æ–‡æœ¬æ•°æ®"""
    if pd.isna(text) or text is None:
        return None
    
    text = str(text).strip()
    return text if text else None

def extract_phone_numbers(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰ç”µè¯å·ç """
    if not text or pd.isna(text):
        return []
    
    text = str(text).strip()
    
    # æå–11ä½æ‰‹æœºå·
    phone_pattern = r'1[3-9]\d{9}'
    phones = re.findall(phone_pattern, text)
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    unique_phones = []
    for phone in phones:
        if phone not in seen:
            seen.add(phone)
            unique_phones.append(phone)
    
    return unique_phones

def extract_name_from_phone_text(text: str) -> Optional[str]:
    """ä»åŒ…å«ç”µè¯å·ç çš„æ–‡æœ¬ä¸­æå–å§“å"""
    if not text or pd.isna(text):
        return None
    
    text = str(text).strip()
    
    # ç§»é™¤ç”µè¯å·ç 
    text_without_phone = re.sub(r'1[3-9]\d{9}', '', text)
    # ç§»é™¤å…¶ä»–æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡
    name_text = re.sub(r'[0-9\s\-\+\(\)\/]+', '', text_without_phone).strip()
    
    return name_text if name_text else None

def clean_ktt_name(name: str) -> Optional[str]:
    """æ¸…ç†KTTåå­—"""
    if not name or pd.isna(name):
        return None
    
    name = str(name).strip()
    
    # ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
    name = re.sub(r'\s+', ' ', name)
    
    # å¦‚æœæ˜¯è¡¨å¤´ï¼Œè¿”å›None
    if name in ['kttåå­—', 'KTTåå­—']:
        return None
    
    return name if name else None

def clean_wechat_name(name: str) -> Optional[str]:
    """æ¸…ç†å¾®ä¿¡åå­—"""
    if not name or pd.isna(name):
        return None
    
    name = str(name).strip()
    
    # ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
    name = re.sub(r'\s+', ' ', name)
    
    # å¦‚æœæ˜¯è¡¨å¤´ï¼Œè¿”å›None
    if name in ['å¾®ä¿¡åå­—', 'å¾®ä¿¡å']:
        return None
    
    return name if name else None

def clean_wechat_id(wechat_id: str) -> Optional[str]:
    """æ¸…ç†å¾®ä¿¡å·"""
    if not wechat_id or pd.isna(wechat_id):
        return None
    
    wechat_id = str(wechat_id).strip()
    
    # ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
    wechat_id = re.sub(r'\s+', ' ', wechat_id)
    
    # å¦‚æœæ˜¯è¡¨å¤´ï¼Œè¿”å›None
    if wechat_id in ['å¾®ä¿¡å·', 'å¾®ä¿¡ID']:
        return None
    
    return wechat_id if wechat_id else None

def clean_address(address: str) -> Optional[str]:
    """æ¸…ç†åœ°å€ä¿¡æ¯"""
    if not address or pd.isna(address):
        return None
    
    address = str(address).strip()
    
    # ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
    address = re.sub(r'\s+', ' ', address)
    
    # å¦‚æœæ˜¯è¡¨å¤´ï¼Œè¿”å›None
    if address in ['ä¸‹å•åœ°å€1', 'ä¸‹å•åœ°å€2', 'åœ°å€1', 'åœ°å€2']:
        return None
    
    return address if address else None

def clean_blacklist_reason(reason: str) -> Optional[str]:
    """æ¸…ç†é»‘åå•åŸå› """
    if not reason or pd.isna(reason):
        return None
    
    reason = str(reason).strip()
    
    # ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
    reason = re.sub(r'\s+', ' ', reason)
    
    # å¦‚æœæ˜¯è¡¨å¤´ï¼Œè¿”å›None
    if reason in ['å…¥é»‘åå•åŸå› ', 'é»‘åå•åŸå› ', 'åŸå› ']:
        return None
    
    return reason if reason else None

def determine_risk_level(reason: str) -> str:
    """æ ¹æ®é»‘åå•åŸå› ç¡®å®šé£é™©ç­‰çº§"""
    if not reason:
        return 'medium'
    
    reason_lower = reason.lower()
    
    # é«˜é£é™©å…³é”®è¯
    high_risk_keywords = [
        'æƒ¯çŠ¯', 'é‡ç‚¹', 'ä¸“ä¸š', 'éª—å­', 'è¯ˆéª—', 'æ¶æ„', 'å¼ºåˆ¶', 'éœ¸ç‹é¤',
        'æ­»ç½ª', 'é”¤æ­»', 'ä¸ä¸‹åå®¶', 'ä¸ä¸‹äº”å®¶', 'ä¸ä¸‹ä¸‰å®¶', 'å››å®¶', 'ä¸‰å®¶'
    ]
    
    # ä½é£é™©å…³é”®è¯
    low_risk_keywords = [
        'è½»å¾®', 'æé†’', 'äº‹ç²¾', 'äº‹å¦ˆ', 'æŒ‘åˆº', 'æ‰¾èŒ¬', 'å°‘å‘', 'æ¼å‘'
    ]
    
    # æ£€æŸ¥é«˜é£é™©å…³é”®è¯
    for keyword in high_risk_keywords:
        if keyword in reason_lower:
            return 'high'
    
    # æ£€æŸ¥ä½é£é™©å…³é”®è¯
    for keyword in low_risk_keywords:
        if keyword in reason_lower:
            return 'low'
    
    return 'medium'

def clean_and_organize_data(file_path: str) -> pd.DataFrame:
    """æ¸…æ´—å’Œæ•´ç†æ•°æ®"""
    print("="*80)
    print("é»‘åå•æ•°æ®æ¸…æ´—å’Œæ•´ç†")
    print("="*80)
    
    try:
        # è¯»å–Excelæ–‡ä»¶
        print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path}")
        df = pd.read_excel(file_path)
        
        # è·³è¿‡ç¬¬ä¸€è¡Œè¯´æ˜æ–‡å­—
        df = df.iloc[1:].reset_index(drop=True)
        print(f"è·³è¿‡ç¬¬ä¸€è¡Œåï¼Œå‰©ä½™è¡Œæ•°: {len(df)}")
        
        # è®¾ç½®æ­£ç¡®çš„åˆ—å
        expected_columns = ['kttåå­—', 'å¾®ä¿¡åå­—', 'å¾®ä¿¡å·', 'ä¸‹å•åå­—å’Œç”µè¯', 'ä¸‹å•åœ°å€1', 'ä¸‹å•åœ°å€2', 'å…¥é»‘åå•åŸå› ']
        df.columns = expected_columns + list(df.columns[len(expected_columns):])
        
        print(f"è®¾ç½®åˆ—åä¸º: {expected_columns}")
        
        # åˆ é™¤å®Œå…¨ç©ºè¡Œ
        initial_rows = len(df)
        df = df.dropna(how='all').reset_index(drop=True)
        empty_rows_removed = initial_rows - len(df)
        print(f"åˆ é™¤äº† {empty_rows_removed} ä¸ªå®Œå…¨ç©ºè¡Œ")
        
        # æ•°æ®æ¸…æ´—
        print("\nå¼€å§‹æ•°æ®æ¸…æ´—...")
        
        # æ¸…æ´—KTTåå­—
        df['kttåå­—_æ¸…æ´—'] = df['kttåå­—'].apply(clean_ktt_name)
        
        # æ¸…æ´—å¾®ä¿¡åå­—
        df['å¾®ä¿¡åå­—_æ¸…æ´—'] = df['å¾®ä¿¡åå­—'].apply(clean_wechat_name)
        
        # æ¸…æ´—å¾®ä¿¡å·
        df['å¾®ä¿¡å·_æ¸…æ´—'] = df['å¾®ä¿¡å·'].apply(clean_wechat_id)
        
        # å¤„ç†ä¸‹å•åå­—å’Œç”µè¯
        df['ä¸‹å•åå­—å’Œç”µè¯_åŸå§‹'] = df['ä¸‹å•åå­—å’Œç”µè¯'].apply(clean_text)
        df['æå–çš„å§“å'] = df['ä¸‹å•åå­—å’Œç”µè¯_åŸå§‹'].apply(extract_name_from_phone_text)
        df['æå–çš„ç”µè¯å·ç '] = df['ä¸‹å•åå­—å’Œç”µè¯_åŸå§‹'].apply(extract_phone_numbers)
        df['ä¸»è¦ç”µè¯å·ç '] = df['æå–çš„ç”µè¯å·ç '].apply(lambda x: x[0] if x else None)
        
        # æ¸…æ´—åœ°å€
        df['ä¸‹å•åœ°å€1_æ¸…æ´—'] = df['ä¸‹å•åœ°å€1'].apply(clean_address)
        df['ä¸‹å•åœ°å€2_æ¸…æ´—'] = df['ä¸‹å•åœ°å€2'].apply(clean_address)
        
        # æ¸…æ´—é»‘åå•åŸå› 
        df['å…¥é»‘åå•åŸå› _æ¸…æ´—'] = df['å…¥é»‘åå•åŸå› '].apply(clean_blacklist_reason)
        df['é£é™©ç­‰çº§'] = df['å…¥é»‘åå•åŸå› _æ¸…æ´—'].apply(determine_risk_level)
        
        # è¿‡æ»¤æœ‰æ•ˆè®°å½•ï¼ˆå¿…é¡»æœ‰KTTåå­—æˆ–ç”µè¯å·ç ï¼‰
        valid_mask = (
            df['kttåå­—_æ¸…æ´—'].notna() | 
            df['ä¸»è¦ç”µè¯å·ç '].notna()
        )
        df_valid = df[valid_mask].copy().reset_index(drop=True)
        
        print(f"æœ‰æ•ˆè®°å½•æ•°: {len(df_valid)} (åŸå§‹: {len(df)})")
        
        # å»é‡å¤„ç†ï¼ˆåŸºäºç”µè¯å·ç ï¼‰
        print("\nå¤„ç†é‡å¤æ•°æ®...")
        phone_counts = df_valid['ä¸»è¦ç”µè¯å·ç '].value_counts()
        duplicate_phones = phone_counts[phone_counts > 1].index.tolist()
        print(f"å‘ç° {len(duplicate_phones)} ä¸ªé‡å¤ç”µè¯å·ç ")
        
        # å¯¹äºé‡å¤çš„ç”µè¯å·ç ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªè®°å½•
        df_deduplicated = df_valid.drop_duplicates(subset=['ä¸»è¦ç”µè¯å·ç '], keep='first').reset_index(drop=True)
        duplicates_removed = len(df_valid) - len(df_deduplicated)
        print(f"å»é‡åè®°å½•æ•°: {len(df_deduplicated)} (åˆ é™¤äº† {duplicates_removed} ä¸ªé‡å¤è®°å½•)")
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_original': len(df),
            'empty_rows_removed': empty_rows_removed,
            'valid_records': len(df_valid),
            'duplicates_removed': duplicates_removed,
            'final_records': len(df_deduplicated),
            'with_ktt_name': df_deduplicated['kttåå­—_æ¸…æ´—'].notna().sum(),
            'with_phone': df_deduplicated['ä¸»è¦ç”µè¯å·ç '].notna().sum(),
            'with_wechat_name': df_deduplicated['å¾®ä¿¡åå­—_æ¸…æ´—'].notna().sum(),
            'with_wechat_id': df_deduplicated['å¾®ä¿¡å·_æ¸…æ´—'].notna().sum(),
            'with_address1': df_deduplicated['ä¸‹å•åœ°å€1_æ¸…æ´—'].notna().sum(),
            'with_address2': df_deduplicated['ä¸‹å•åœ°å€2_æ¸…æ´—'].notna().sum(),
            'with_reason': df_deduplicated['å…¥é»‘åå•åŸå› _æ¸…æ´—'].notna().sum(),
            'risk_levels': df_deduplicated['é£é™©ç­‰çº§'].value_counts().to_dict()
        }
        
        print(f"\nğŸ“Š æ¸…æ´—åæ•°æ®ç»Ÿè®¡:")
        print(f"  åŸå§‹è®°å½•æ•°: {stats['total_original']}")
        print(f"  åˆ é™¤ç©ºè¡Œ: {stats['empty_rows_removed']}")
        print(f"  æœ‰æ•ˆè®°å½•: {stats['valid_records']}")
        print(f"  åˆ é™¤é‡å¤: {stats['duplicates_removed']}")
        print(f"  æœ€ç»ˆè®°å½•: {stats['final_records']}")
        print(f"  æœ‰KTTåå­—: {stats['with_ktt_name']}")
        print(f"  æœ‰ç”µè¯å·ç : {stats['with_phone']}")
        print(f"  æœ‰å¾®ä¿¡åå­—: {stats['with_wechat_name']}")
        print(f"  æœ‰å¾®ä¿¡å·: {stats['with_wechat_id']}")
        print(f"  æœ‰åœ°å€1: {stats['with_address1']}")
        print(f"  æœ‰åœ°å€2: {stats['with_address2']}")
        print(f"  æœ‰é»‘åå•åŸå› : {stats['with_reason']}")
        print(f"  é£é™©ç­‰çº§åˆ†å¸ƒ: {stats['risk_levels']}")
        
        # æ˜¾ç¤ºæ ·æœ¬æ•°æ®
        print(f"\nğŸ“‹ æ¸…æ´—åæ ·æœ¬æ•°æ® (å‰5æ¡):")
        sample_columns = [
            'kttåå­—_æ¸…æ´—', 'å¾®ä¿¡åå­—_æ¸…æ´—', 'å¾®ä¿¡å·_æ¸…æ´—', 
            'æå–çš„å§“å', 'ä¸»è¦ç”µè¯å·ç ', 'ä¸‹å•åœ°å€1_æ¸…æ´—', 
            'å…¥é»‘åå•åŸå› _æ¸…æ´—', 'é£é™©ç­‰çº§'
        ]
        
        for index, row in df_deduplicated.head(5).iterrows():
            print(f"\n  è®°å½• {index + 1}:")
            for col in sample_columns:
                if col in df_deduplicated.columns:
                    value = row[col]
                    if pd.notna(value):
                        print(f"    {col}: {value}")
                    else:
                        print(f"    {col}: [ç©º]")
        
        # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        output_file = "data/blacklist/cleaned_blacklist_data.xlsx"
        df_deduplicated.to_excel(output_file, index=False)
        print(f"\nâœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        stats_file = "data/blacklist/data_cleaning_report.json"
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        stats_serializable = {}
        for key, value in stats.items():
            if isinstance(value, dict):
                stats_serializable[key] = {k: int(v) if isinstance(v, (np.integer, np.int64)) else v for k, v in value.items()}
            elif isinstance(value, (np.integer, np.int64)):
                stats_serializable[key] = int(value)
            else:
                stats_serializable[key] = value
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats_serializable, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {stats_file}")
        
        return df_deduplicated
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ¸…æ´—è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    excel_file = "data/blacklist/å‰¯æœ¬-5_kttæ‰‹ä½œéª—å­æŒæ›´2025ç‰ˆ.xlsx"
    
    if not Path(excel_file).exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {excel_file}")
        return
    
    result = clean_and_organize_data(excel_file)
    
    if result is not None:
        print(f"\nâœ… æ•°æ®æ¸…æ´—å’Œæ•´ç†å®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆå¤„ç†äº† {len(result)} æ¡æœ‰æ•ˆè®°å½•")
    else:
        print(f"\nâŒ æ•°æ®æ¸…æ´—å¤±è´¥")

if __name__ == "__main__":
    main()
